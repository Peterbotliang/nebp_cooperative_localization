import os
import sys
sys.path.append(os.path.dirname(__file__))

import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as func
from torch.utils.data import Dataset, DataLoader

import dgl
import dgl.function as fn

from tqdm import tqdm

from synthetic import synthetic_dataset, get_model_parameters
from resample import resample_systematic
from sqrtm import sqrtm_sym

class cooperative_localization_joint_solver(nn.Module):

    def __init__(self, F, Q, sigma_meas,
                 num_agents = 3,
                 num_particles = 1500):

        super(cooperative_localization_joint_solver, self).__init__()

        self.F, self.Q = F, Q
        self.sigma_meas = sigma_meas

        if isinstance(F, np.ndarray):
            self.F = torch.from_numpy(F.astype(np.float32))
        if isinstance(Q, np.ndarray):
            self.Q = torch.from_numpy(Q.astype(np.float32))

        self.num_agents = num_agents
        self.num_particles = num_particles

        self.dim_state, _ = self.F.shape
        self.dim_state_joint = self.num_agents * self.dim_state
        self.bigF = torch.zeros(self.dim_state_joint, self.dim_state_joint)
        self.bigQ = torch.zeros(self.dim_state_joint, self.dim_state_joint)

        for i in range(self.num_agents):
            start = i * self.dim_state
            end = (i + 1) * self.dim_state
            self.bigF[start : end, start : end] = self.F
            self.bigQ[start : end, start : end] = self.Q


    def _apply(self, fn):
        super(cooperative_localization_joint_solver, self)._apply(fn)
        self.F = fn(self.F)
        self.Q = fn(self.Q)
        self.bigF = fn(self.bigF)
        self.bigQ = fn(self.bigQ)
        return self

    def init_particles(self, x_prior, P0):
        rv = torch.distributions.multivariate_normal.MultivariateNormal(loc = torch.zeros_like(x_prior), covariance_matrix = P0)
        particles = (x_prior.unsqueeze(2) + rv.rsample(sample_shape = (self.num_particles,)).permute(1, 2, 0))

        return particles

    def calculate_weight_anchor(self, nodes):
        anchor_pos_agent = nodes.data['anchor_pos']
        meas = nodes.data['meas']
        mask = nodes.data['mask']
        particles = nodes.data['particles']

        num_nodes, num_max_parter_anchor, _ = anchor_pos_agent.shape

        predicted_meas = torch.sum((particles[:, : 2, :].unsqueeze(3) - anchor_pos_agent.transpose(1, 2).unsqueeze(2).contiguous()) ** 2, dim = 1) ** 0.5
        weights_log = torch.sum((-(predicted_meas - meas.unsqueeze(1)) ** 2 / (2 * self.sigma_meas ** 2)) * mask.unsqueeze(1), dim = 2)
        weights_log = weights_log - torch.max(weights_log, dim = 1)[0].unsqueeze(1)

        return {'weights_anchor_log': weights_log}

    def calculate_weight_agent(self, edges):
        particles_dst = edges.dst['particles']
        particles_src = edges.src['particles']
        meas = edges.data['meas']
        belief_src_log = edges.src['belief_log']

        # predicted_meas = torch.sum((particles_dst[:, : 2, :].unsqueeze(3) - particles_src[:, : 2, :].unsqueeze(2)) ** 2, dim = 1) ** 0.5
        # weights = torch.sum(torch.exp(-(predicted_meas - meas.unsqueeze(1)) ** 2 / (2 * self.sigma_meas ** 2)) * torch.exp(belief_src_log).unsqueeze(1), dim = 2)

        predicted_meas = torch.sum((particles_dst[:, : 2, :] - particles_src[:, : 2, :]) ** 2, dim = 1) ** 0.5
        # weights = torch.sum(torch.exp(-(predicted_meas - meas.unsqueeze(1)) ** 2 / (2 * self.sigma_meas ** 2)) * torch.exp(belief_src_log).unsqueeze(1), dim = 2)
        # weights_log = torch.log(weights + torch.finfo(weights.dtype).eps)
        weights_log = -(predicted_meas - meas) ** 2 / (2 * self.sigma_meas ** 2)
        weights_log = weights_log - torch.max(weights_log, dim = 1)[0].unsqueeze(1)

        return {'weights_agent_log': weights_log}

    def calculate_belief(self, nodes):
        return {'belief_log': nodes.data['weights_anchor_log'] + torch.sum(nodes.mailbox['weights_agent_log'], dim = 1)}


    # def calculate_sp_meas_anchor(self, nodes):
    #     anchor_pos_agent = nodes.data['anchor_pos']
    #     meas_anchor = nodes.data['meas']
    #     mask = nodes.data['mask']
    #     sp = nodes.data['sp']

    #     sp_meas_anchor = torch.sum((sp[:, : 2, :].unsqueeze(3) - anchor_pos_agent.transpose(1, 2).unsqueeze(2).contiguous()) ** 2, dim = 1) ** 0.5 * mask.unsqueeze(1)

    def calculate_sp_meas_agent(self, edges):
        sp_dst = edges.dst['sp']
        sp_src = edges.src['sp']

        sp_meas_agent = torch.sum((sp_dst[:, : 2, :] - sp_src[:, : 2, :]) ** 2, dim = 1) ** 0.5

        return {'sp_meas_agent': sp_meas_agent, 'meas_agent': edges.data['meas']}

    def aggregate_sp_meas(self, nodes):
        anchor_pos_agent = nodes.data['anchor_pos']
        meas_anchor = nodes.data['meas']
        mask = nodes.data['mask']
        sp = nodes.data['sp']

        num_nodes, _, _ = anchor_pos_agent.shape

        if nodes.mailbox is None:
            sp_meas_agent_ = torch.zeros(num_nodes, self.num_agents, 2 * self.dim_state_joint + 1, device = sp.device)
            meas_agent_ = torch.zeros(num_nodes, self.num_agents, 1, device = sp.device)
        else:
            sp_meas_agent_ = nodes.mailbox['sp_meas_agent']
            meas_agent_ = nodes.mailbox['meas_agent']

        _, num_partner, _, = sp_meas_agent_.shape

        sp_meas_agent = torch.zeros(num_nodes, self.num_agents - 1, 2 * self.dim_state_joint + 1, device = sp_meas_agent_.device)
        meas_agent = torch.zeros(num_nodes, self.num_agents - 1, 1, device = meas_agent_.device)
        sp_meas_agent[:, :num_partner, :] = sp_meas_agent_
        meas_agent[:, :num_partner, :] = meas_agent_

        sp_meas_anchor = torch.sum((sp[:, : 2, :].unsqueeze(3) - anchor_pos_agent.transpose(1, 2).unsqueeze(2).contiguous()) ** 2, dim = 1) ** 0.5 * mask.unsqueeze(1)


        sp_meas = torch.cat([sp_meas_anchor.transpose(1, 2), sp_meas_agent], dim = 1)
        meas = torch.cat([meas_anchor, meas_agent.squeeze(2)], dim = 1)

        return {'sp_meas' : sp_meas, 'sp_meas_true' : meas}

    def perform_estimation(self, g, mean, particles, cov, filter_mode = 1, num_iter = 1):
        assert mean.shape[1] == self.bigF.shape[1]
        assert filter_mode in [0, 1, 2]
        batch_size, _ = mean.shape
        num_nodes = batch_size * self.num_agents

        with g.local_scope():
            if filter_mode == 1:
                # -----------------------------
                # Gaussian prediction step
                # -----------------------------
                predicted_mean, predicted_cov = self.gaussian_predict(mean, cov)

                # -----------------------------
                # sigma point update
                # -----------------------------
                Lambda = 0
                weights_sp = torch.ones(2 * self.dim_state_joint + 1, device = mean.device) / (2 * (self.dim_state_joint + Lambda))
                weights_sp[0] = Lambda

                sp = torch.cat([predicted_mean.unsqueeze(-1),
                                predicted_mean.unsqueeze(-1) + ((self.dim_state_joint + Lambda) ** 0.5) * sqrtm_sym(predicted_cov),
                                predicted_mean.unsqueeze(-1) - ((self.dim_state_joint + Lambda) ** 0.5) * sqrtm_sym(predicted_cov)], dim = -1)

                g.ndata['sp'] = sp.reshape(batch_size * self.num_agents, self.dim_state, 2 * self.dim_state_joint + 1)

                if g.num_edges() != 0:
                    g.update_all(self.calculate_sp_meas_agent, self.aggregate_sp_meas)
                else:
                    g.apply_nodes(self.aggregate_sp_meas)

                sp_meas = g.ndata['sp_meas'].reshape(batch_size, -1, 2 * self.dim_state_joint + 1)
                sp_meas_true = g.ndata['sp_meas_true'].reshape(batch_size, -1)

                sp_meas_mean = torch.mean(sp_meas, dim = 2)
                sp_meas_zero_mean = sp_meas - sp_meas_mean.unsqueeze(2)
                sp_mean = torch.mean(sp, dim = 2)
                sp_zero_mean = sp - sp_mean.unsqueeze(2)

                noise_cov = torch.eye(sp_meas_true.shape[1], device = mean.device) * self.sigma_meas ** 2
                sp_meas_cov = torch.matmul(sp_meas_zero_mean * weights_sp, sp_meas_zero_mean.transpose(1, 2))
                sp_meas_cov = sp_meas_cov + noise_cov

                sp_cross_cov = torch.matmul(sp_zero_mean * weights_sp, sp_meas_zero_mean.transpose(1, 2))

                kalman_gain = torch.matmul(sp_cross_cov, torch.inverse(sp_meas_cov))
                updated_mean = predicted_mean + torch.matmul(kalman_gain, (sp_meas_true - sp_meas_mean).unsqueeze(2)).squeeze(2)
                updated_cov = predicted_cov - torch.matmul(torch.matmul(kalman_gain, sp_meas_cov), kalman_gain.transpose(1, 2))
                updated_cov, updated_u = self.fix_cov(updated_cov)
                # assert torch.all(torch.eq(updated_cov, updated_cov.transpose(-2, -1)))

                # -----------------------------
                # particle-based update
                # -----------------------------
                rv_predict = torch.distributions.multivariate_normal.MultivariateNormal(loc = predicted_mean, covariance_matrix = predicted_cov)
                rv_update = torch.distributions.multivariate_normal.MultivariateNormal(loc = updated_mean, covariance_matrix = updated_cov)
                particles = rv_update.rsample(sample_shape = (self.num_particles,))
                weights_proposal_log = (rv_predict.log_prob(particles) - rv_update.log_prob(particles)).transpose(0, 1)
                particles = particles.permute(1, 2, 0)

            elif filter_mode == 2:
                particles = self.particle_predict(particles)
                weights_proposal_log = 0

            g.ndata['particles'] = particles.reshape(num_nodes, self.dim_state, self.num_particles)
            g.ndata['belief_log'] = torch.zeros(num_nodes, self.num_particles, device = particles.device)

            g.apply_nodes(self.calculate_weight_anchor)
            for _ in range(num_iter):
                g.update_all(self.calculate_weight_agent, self.calculate_belief)
            if g.num_edges() == 0:
                g.ndata['belief_log'] = g.ndata['weights_anchor_log']

            belief_log = torch.sum(g.ndata['belief_log'].reshape(batch_size, self.num_agents, self.num_particles), dim = 1) + weights_proposal_log
            belief = torch.exp(belief_log - torch.max(belief_log, dim = 1)[0].unsqueeze(1))
            belief = belief / torch.sum(belief, dim = 1).unsqueeze(1)

            # estimate mean and covariance
            estimated_mean = torch.sum(particles * belief.unsqueeze(1), dim = 2)
            # particles_zero_mean = particles - estimated_mean.unsqueeze(2)
            # estimated_cov = torch.matmul(particles_zero_mean * belief.unsqueeze(1), particles_zero_mean.transpose(1, 2))

            ind = torch.multinomial(belief, self.num_particles, replacement = True)
            particles_resampled = torch.gather(particles, dim = 2, index = ind.unsqueeze(1).expand(-1, self.dim_state_joint, -1))

            estimated_mean = torch.sum(particles * belief.unsqueeze(1), dim = 2)
            particles_zero_mean = particles_resampled - estimated_mean.unsqueeze(2)
            estimated_cov = torch.matmul(particles_zero_mean, particles_zero_mean.transpose(1, 2)) / self.num_particles

            return estimated_mean, particles_resampled, estimated_cov

    def fix_cov(self, cov):
        cov_ = cov.clone()
        if not torch.isreal(cov_).all():
            cov_ = torch.real(cov_)

        if torch.isnan(cov_).any():
            cov_[torch.isnan(cov_)] = 0

        cov_ = sqrtm_sym(torch.matmul(cov_, cov_.transpose(-2, -1)))
        cov_ = cov_ + torch.eye(self.dim_state_joint, device = cov_.device) * 1e-3
        try:
            U = torch.cholesky(cov_)
        except RuntimeError:
            cov_ = cov_ + torch.eye(self.dim_state_joint, device = cov_.device) * 1e-3

        return cov_, U


    def gaussian_predict(self, mean, cov):
        num_nodes, _ = mean.shape

        predicted_mean = torch.matmul(self.bigF, mean.unsqueeze(2))
        predicted_cov = torch.matmul(torch.matmul(self.bigF, cov), self.bigF.transpose(0, 1)) + self.bigQ

        try:
            U = torch.cholesky(predicted_cov)
        except RuntimeError:
            predicted_cov = predicted_cov + torch.eye(self.dim_state_joint, device = predicted_cov.device) * 1e-3

        return predicted_mean.squeeze(-1), predicted_cov

    def particle_predict(self, particles):
        batch_size, _, num_particles = particles.shape

        rv = torch.distributions.multivariate_normal.MultivariateNormal(torch.matmul(particles.transpose(1, 2), self.bigF.transpose(0, 1)).transpose(1, 2).permute(2, 0, 1), covariance_matrix = self.bigQ)

        # particles_proposal = torch.matmul(particles.transpose(1, 2), self.bigF.transpose(0, 1)).transpose(1, 2) + rv.rsample().permute(1, 2, 0)
        particles_proposal = rv.rsample().permute(1, 2, 0)

        return particles_proposal.to(particles.device)
